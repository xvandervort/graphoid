#!/usr/bin/env glang
# Demonstration of pure Glang HTML processing capabilities
# Shows how much can be done without Python dependency

print("ðŸš€ Pure Glang HTML Processing Demo")
print("==================================================")

# Sample HTML content for testing
sample_html = "<html><head><title>Pure Glang HTML Demo</title><meta name=\"description\" content=\"Testing Glang HTML processing\"><meta property=\"og:title\" content=\"Glang Rules!\"></head><body><h1>Welcome to Glang</h1><p>This is a <strong>test</strong> paragraph with <a href=\"https://example.com\">a link</a>.</p><p>Contact us at: support@glang.dev or admin@glang.dev</p><table><tr><th>Feature</th><th>Status</th><th>Performance</th></tr><tr><td>String Processing</td><td>Complete</td><td>Fast</td></tr><tr><td>HTML Parsing</td><td>Working</td><td>Good</td></tr><tr><td>Graph Computing</td><td>In Progress</td><td>Excellent</td></tr></table><div><img src=\"/images/logo.png\" alt=\"Glang Logo\"><img src=\"/images/banner.jpg\" alt=\"Banner\"><script src=\"/js/app.js\"></script></div></body></html>"

# Test pure Glang functions (no Python needed!)
print("\n1. HTML Entity Encoding/Decoding (Pure Glang):")
test_text = '<script>alert("XSS")</script>'
encoded = test_text.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;").replace('"', "&quot;")
print("Original: " + test_text)
print("Encoded:  " + encoded)
decoded = encoded.replace("&lt;", "<").replace("&gt;", ">").replace("&quot;", '"').replace("&amp;", "&")
print("Decoded:  " + decoded)

print("\n2. Strip HTML Tags (Pure Glang):")
paragraph = '<p>This is a <strong>test</strong> with <em>formatting</em>.</p>'
# Simple tag stripping
text = paragraph
while text.contains("<") and text.contains(">") {
    start = text.split("<")[0]
    rest = text.split(">")
    if rest.size() > 1 {
        text = start + rest[1:].join(">")
    } else {
        break
    }
}
print("HTML: " + paragraph)
print("Text: " + text)

print("\n3. Extract URLs (Pure Glang):")
urls = []

# Extract href URLs
href_parts = sample_html.split('href="')
i = 1
while i < href_parts.size() {
    url = href_parts[i].split('"')[0]
    if url != "" and url != "#" {
        urls.append("Link: " + url)
    }
    i = i + 1
}

# Extract src URLs
src_parts = sample_html.split('src="')
i = 1
while i < src_parts.size() {
    url = src_parts[i].split('"')[0]
    if url != "" {
        urls.append("Resource: " + url)
    }
    i = i + 1
}

print("Found " + urls.size().to_string() + " URLs:")
i = 0
while i < urls.size() {
    print("  - " + urls[i])
    i = i + 1
}

print("\n4. Extract Title (Pure Glang):")
title_start = sample_html.split("<title>")
if title_start.size() > 1 {
    title = title_start[1].split("</title>")[0]
    print("Page Title: " + title)
}

print("\n5. Extract Meta Tags (Pure Glang):")
meta_tags = sample_html.split("<meta")
i = 1
while i < meta_tags.size() {
    tag = meta_tags[i].split(">")[0]

    # Extract name/property
    name = ""
    if tag.contains('name="') {
        name = tag.split('name="')[1].split('"')[0]
    } else if tag.contains('property="') {
        name = tag.split('property="')[1].split('"')[0]
    }

    # Extract content
    if tag.contains('content="') {
        content = tag.split('content="')[1].split('"')[0]
        if name != "" {
            print("  " + name + ": " + content)
        }
    }

    i = i + 1
}

print("\n6. Extract Table Data (Pure Glang):")
# Extract table rows
table_html = sample_html.split("<table>")[1].split("</table>")[0]
rows = table_html.split("<tr>")
table_data = []

r = 1  # Skip empty first element
while r < rows.size() {
    row = rows[r].split("</tr>")[0]
    cells = []

    # Try header cells first
    if row.contains("<th>") {
        th_parts = row.split("<th>")
        c = 1
        while c < th_parts.size() {
            cell = th_parts[c].split("</th>")[0]
            cells.append(cell)
            c = c + 1
        }
    } else {
        # Regular cells
        td_parts = row.split("<td>")
        c = 1
        while c < td_parts.size() {
            cell = td_parts[c].split("</td>")[0]
            cells.append(cell)
            c = c + 1
        }
    }

    if cells.size() > 0 {
        table_data.append(cells)
    }
    r = r + 1
}

print("Table has " + table_data.size().to_string() + " rows:")
r = 0
while r < table_data.size() {
    row = table_data[r]
    row_text = " | ".join(row)
    print("  Row " + (r + 1).to_string() + ": " + row_text)
    r = r + 1
}

print("\n7. Find Email Addresses (Pure Glang with Regex):")
import "regex"    # Can use as 're' or 'regex'
# Extract text without tags first
clean_text = sample_html
while clean_text.contains("<") and clean_text.contains(">") {
    parts = clean_text.split("<")
    kept = [parts[0]]
    i = 1
    while i < parts.size() {
        after_tag = parts[i].split(">")
        if after_tag.size() > 1 {
            kept.append(after_tag[1:].join(">"))
        }
        i = i + 1
    }
    new_text = kept.join("")
    if new_text == clean_text {
        break
    }
    clean_text = new_text
}

# Find emails with regex
email_pattern = "[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}"
emails = re.findAll(clean_text, email_pattern)
print("Found " + emails.size().to_string() + " email addresses:")
i = 0
while i < emails.size() {
    print("  - " + emails[i])
    i = i + 1
}

print("\n8. Extract All Headings (Pure Glang):")
heading_levels = ["h1", "h2", "h3", "h4", "h5", "h6"]
i = 0
while i < heading_levels.size() {
    tag = heading_levels[i]
    start_tag = "<" + tag + ">"
    end_tag = "</" + tag + ">"

    if sample_html.contains(start_tag) {
        parts = sample_html.split(start_tag)
        j = 1
        while j < parts.size() {
            if parts[j].contains(end_tag) {
                heading_text = parts[j].split(end_tag)[0]
                print("  " + tag.upper() + ": " + heading_text)
            }
            j = j + 1
        }
    }
    i = i + 1
}

print("\n" + "==================================================")
print("âœ¨ Demo Complete!")
print("\nThis demo shows that Glang can do extensive HTML processing")
print("using only its native string manipulation capabilities!")
print("\nPython is only needed for complex DOM parsing, not for:")
print("  - URL extraction")
print("  - Table data extraction")
print("  - Meta tag parsing")
print("  - Email finding")
print("  - Text extraction")
print("  - Entity encoding/decoding")