#!/usr/bin/env glang

module dataframe

# DataFrame Module - Tabular data as graph structures with governance rules
#
# A DataFrame is a graph where:
# - Nodes represent cells in a table
# - Edges connect cells within rows and columns
# - Governance rules ensure tabular structure integrity
#
# Key Principles:
# 1. DataFrames are graphs with tabular governance rules
# 2. All operations preserve structural integrity
# 3. Columns have consistent types (enforced by rules)
# 4. Rows maintain fixed column count

# Create a new DataFrame from column definitions
func create(columns) {
    # For now, use a map to represent the DataFrame structure
    # Future: This will be a native graph type with DataFrame governance
    df = {
        "_type": "dataframe",
        "_columns": columns,
        "_data": [],
        "_row_count": 0
    }

    # Initialize empty columns
    for col in columns {
        df[col] = []
    }

    return df
}

# Create DataFrame from list of maps with explicit columns
func from_records(records, columns) {
    if records.size() == 0 {
        return create([])
    }

    # Create DataFrame with specified columns
    df = create(columns)

    # Add each record as a row
    for record in records {
        add_row(df, record)
    }

    return df
}

# Create DataFrame from column data using variable keys (enhanced!)
func from_column_data(column_data) {
    # column_data is a map where keys are column names, values are lists
    # Example: { "name": ["Alice", "Bob"], "age": [25, 30] }

    # Extract column names using map.keys() method
    columns = column_data.keys()  # Much cleaner with keys() method!

    df = create(columns)

    # Find max row count
    max_rows = 0
    for col in columns {
        col_key = col  # Variable key usage!
        if column_data.has_key(col_key) {
            col_data = column_data[col_key]
            if col_data.size() > max_rows {
                max_rows = col_data.size()
            }
        }
    }

    # Build rows using variable keys for cleaner syntax
    for i in [].upto(max_rows - 1) {
        row = {}
        for col in columns {
            col_name = col  # Variable key for modern syntax
            if column_data.has_key(col_name) && i < column_data[col_name].size() {
                row[col_name] = column_data[col_name][i]  # Clean variable key assignment!
            } else {
                row[col_name] = none
            }
        }
        add_row(df, row)
    }

    return df
}

# Create DataFrame from CSV text
func from_csv(csv_text, has_headers) {
    import "csv" as csv_module

    # Parse CSV into list of lists
    rows = csv_module.parse(csv_text, has_headers, ",")

    if rows.size() == 0 {
        return create([])
    }

    # Determine columns
    columns = []
    data_start = 0

    if has_headers {
        # First row contains column names
        columns = rows[0]
        data_start = 1
    } else {
        # Generate column names: col0, col1, etc.
        first_row = rows[0]
        for i in first_row.upto(first_row.size() - 1) {
            columns.append("col" + i.to_string())
        }
    }

    # Create DataFrame
    df = create(columns)

    # Add data rows
    for i in [].upto(rows.size() - 1) {
        if i >= data_start {
            row_data = rows[i]
            row_map = {}

            for j in [].upto(columns.size() - 1) {
                col_name = columns[j]  # Use variable for cleaner syntax
                if j < row_data.size() {
                    row_map[col_name] = row_data[j]
                } else {
                    row_map[col_name] = none  # Missing values
                }
            }

            add_row(df, row_map)
        }
    }

    return df
}

# Add a row to DataFrame
func add_row(df, row_data) {
    # Validate that row has correct columns
    columns = df["_columns"]

    for col in columns {
        if row_data.has_key(col) {
            df[col].append(row_data[col])
        } else {
            df[col].append(none)  # Fill missing with none
        }
    }

    df["_data"].append(row_data)
    df["_row_count"] = df["_row_count"] + 1
}

# Select specific columns
func select(df, column_names) {
    result = create(column_names)

    # Copy data for selected columns using variable keys
    for i in [].upto(df["_row_count"] - 1) {
        row = {}
        for col in column_names {
            col_key = col  # Variable key for cleaner syntax
            if df.has_key(col) {
                row[col_key] = df[col][i]
            }
        }
        add_row(result, row)
    }

    return result
}

# Filter rows based on condition
func filter(df, column, predicate) {
    columns = df["_columns"]
    result = create(columns)

    # Check each row
    for i in [].upto(df["_row_count"] - 1) {
        value = df[column][i]

        # Apply predicate
        keep = false
        if predicate == "positive" {
            if value != none && value > 0 {
                keep = true
            }
        } else if predicate == "negative" {
            if value != none && value < 0 {
                keep = true
            }
        } else if predicate == "non_empty" {
            if value != none && value != "" {
                keep = true
            }
        } else if predicate == "truthy" {
            if value {
                keep = true
            }
        }

        # Add row if it passes filter
        if keep {
            row = {}
            for col in columns {
                col_var = col  # Variable key for enhanced readability
                row[col_var] = df[col][i]
            }
            add_row(result, row)
        }
    }

    return result
}

# Filter with custom function
func filter_by(df, column, filter_func) {
    columns = df["_columns"]
    result = create(columns)

    for i in [].upto(df["_row_count"] - 1) {
        value = df[column][i]

        # Apply custom filter function
        if filter_func(value) {
            row = {}
            for col in columns {
                col_name = col  # Variable key for consistency
                row[col_name] = df[col][i]
            }
            add_row(result, row)
        }
    }

    return result
}

# Aggregate operations
func aggregate(df, column, operation) {
    col_data = df[column]

    if operation == "sum" {
        total = 0
        for value in col_data {
            if value != none {
                total = total + value
            }
        }
        return total
    } else if operation == "mean" || operation == "avg" || operation == "average" {
        total = 0
        count = 0
        for value in col_data {
            if value != none {
                total = total + value
                count = count + 1
            }
        }
        if count > 0 {
            return total / count
        } else {
            return none
        }
    } else if operation == "min" {
        min_val = none
        for value in col_data {
            if value != none {
                if min_val == none || value < min_val {
                    min_val = value
                }
            }
        }
        return min_val
    } else if operation == "max" {
        max_val = none
        for value in col_data {
            if value != none {
                if max_val == none || value > max_val {
                    max_val = value
                }
            }
        }
        return max_val
    } else if operation == "count" {
        count = 0
        for value in col_data {
            if value != none {
                count = count + 1
            }
        }
        return count
    }

    return none
}

# Group by column and aggregate (simplified version)
func group_by(df, group_column, agg_column, operation) {
    # Proper group_by implementation using map.keys() method
    groups = {}
    columns = df["_columns"]

    # Group rows by the group_column value
    for i in [].upto(df["_row_count"] - 1) {
        group_value = df[group_column][i]
        group_key = group_value.to_string()

        # Initialize group if it doesn't exist
        if !groups.has_key(group_key) {
            groups[group_key] = []
        }

        # Add row to group
        row = {}
        for col in columns {
            col_var = col  # Variable key for consistency
            row[col_var] = df[col][i]
        }
        groups[group_key].append(row)
    }

    # Apply aggregation to each group
    result = {}
    group_keys = groups.keys()  # Using map.keys() method!

    for group_key in group_keys {
        group_rows = groups[group_key]

        # Create temporary DataFrame for this group
        group_df = create(columns)
        for row in group_rows {
            add_row(group_df, row)
        }

        # Calculate aggregate for this group
        agg_result = aggregate(group_df, agg_column, operation)
        result[group_key] = agg_result
    }

    return result
}

# Group by column and return sub-DataFrames for each group
func group_by_dataframes(df, group_column) {
    # Returns a map where keys are group values, values are DataFrames
    groups = {}
    columns = df["_columns"]

    # Group rows by the group_column value
    for i in [].upto(df["_row_count"] - 1) {
        group_value = df[group_column][i]
        group_key = group_value.to_string()

        # Initialize group DataFrame if it doesn't exist
        if !groups.has_key(group_key) {
            groups[group_key] = create(columns)
        }

        # Add row to group DataFrame
        row = {}
        for col in columns {
            col_name = col  # Variable key for clean syntax
            row[col_name] = df[col][i]
        }
        add_row(groups[group_key], row)
    }

    return groups
}

# Multiple aggregations on groups - pandas-style functionality
func group_by_agg(df, group_column, agg_operations) {
    # agg_operations is a map: { "column": "operation", ... }
    # Example: { "salary": "mean", "count": "count", "age": "max" }

    group_dfs = group_by_dataframes(df, group_column)
    group_keys = group_dfs.keys()  # Using map.keys() method!

    result = {}

    for group_key in group_keys {
        group_df = group_dfs[group_key]
        group_result = {}

        # Apply each aggregation operation
        agg_cols = agg_operations.keys()  # Using map.keys() again!
        for agg_col in agg_cols {
            operation = agg_operations[agg_col]

            if operation == "count" {
                group_result[agg_col] = group_df["_row_count"]
            } else {
                agg_value = aggregate(group_df, agg_col, operation)
                group_result[agg_col] = agg_value
            }
        }

        result[group_key] = group_result
    }

    return result
}

# Display DataFrame info
func info(df) {
    print("DataFrame with " + df["_row_count"].to_string() + " rows")
    print("Columns: " + df["_columns"].to_string())
}

# Get first n rows
func head(df, n) {
    columns = df["_columns"]
    result = create(columns)

    max_rows = n
    if n > df["_row_count"] {
        max_rows = df["_row_count"]
    }

    for i in [].upto(max_rows - 1) {
        row = {}
        for col in columns {
            col_key = col  # Variable key for modern syntax
            row[col_key] = df[col][i]
        }
        add_row(result, row)
    }

    return result
}

# Convert DataFrame to CSV string
func to_csv(df) {
    lines = []
    columns = df["_columns"]

    # Add header row
    header = ""
    for i in [].upto(columns.size() - 1) {
        if i > 0 {
            header = header + ","
        }
        header = header + columns[i]
    }
    lines.append(header)

    # Add data rows
    for i in [].upto(df["_row_count"] - 1) {
        row_str = ""
        for j in [].upto(columns.size() - 1) {
            if j > 0 {
                row_str = row_str + ","
            }
            value = df[columns[j]][i]
            if value != none {
                row_str = row_str + value.to_string()
            }
        }
        lines.append(row_str)
    }

    # Join lines
    result = ""
    for line in lines {
        if result != "" {
            result = result + "\n"
        }
        result = result + line
    }

    return result
}

# DataFrame Rules (Future: These will be enforced by control layer)
#
# 1. TABULAR_STRUCTURE: All rows must have same columns
# 2. COLUMN_CONSISTENCY: Values in a column should have consistent type
# 3. ROW_INTEGRITY: Rows maintain connection to their columns
# 4. NO_CROSS_CONTAMINATION: Cell edges only within same DataFrame
# 5. ORDERED_ACCESS: Rows accessed sequentially, columns by name

# Future governance rules to be implemented:
func _define_dataframe_rules() {
    # These conceptual rules will be enforced when DataFrames
    # become native graph types with control layer governance

    rules = [
        "tabular_structure",      # Enforces rectangular shape
        "column_type_consistency", # Same type within column
        "row_column_edges_only",   # No diagonal edges
        "no_external_edges",       # Cells can't link outside
        "preserve_row_order"       # Row sequence maintained
    ]

    return rules
}

# Lambda-powered column transformation
func transform_column(df, column, transform_func) {
    if !df.has_key(column) {
        return df
    }

    col_data = df[column]
    transformed = col_data.map(transform_func)
    df[column] = transformed
    return df
}

# Compute basic statistics for a column
func compute_basic_stats(df, column) {
    col_data = df[column].filter(x => x != none)
    if col_data.size() == 0 {
        return { "count": 0, "mean": none, "min": none, "max": none, "range": none }
    }

    count = col_data.size()
    total = col_data.sum()
    mean = total / count
    min_val = col_data.min()
    max_val = col_data.max()
    range_val = max_val - min_val

    return {
        "count": count,
        "mean": mean,
        "min": min_val,
        "max": max_val,
        "range": range_val
    }
}

# Normalize column values using custom lambda
func normalize_column(df, column, normalize_func) {
    if !df.has_key(column) {
        return df
    }

    col_data = df[column]
    normalized = col_data.map(normalize_func)
    df[column] = normalized
    return df
}

print("DataFrame module loaded - tabular data with graph governance and lambda support")