# Web Scraping Demo with HTTP and HTML Parsing
# Demonstrates combining HTTP client with HTML parsing for web scraping

import "http" as http
import "html_parser" as html
import "network" as network

print("ğŸ•·ï¸ Web Scraping Demo")
print("====================")
print("ğŸ”— Combining HTTP client with HTML parsing for data extraction")

# Test if network is available
if network.is_network_available() == false {
    print("âŒ Network not available - cannot perform web scraping demo")
    exit(1)
}

# Configuration
string user_agent = "Glang-WebScraper/1.0"

# Helper function to create web scraping headers
func create_scraping_headers() {
    return {
        "User-Agent": user_agent,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate",
        "Connection": "keep-alive"
    }
}

# Helper function to fetch and parse HTML
func fetch_and_parse(url) {
    print("ğŸ”„ Fetching: " + url)

    response = http.get(url, create_scraping_headers())

    if response.get("success") == false {
        print("âŒ Failed to fetch page: " + response.get("status").to_string())
        return none
    }

    print("âœ… Page fetched successfully (" + response.get("body").length().to_string() + " characters)")

    # Parse HTML
    html_content = response.get("body")
    elements = html.parse(html_content)

    print("ğŸ“Š Parsed " + elements.size().to_string() + " root HTML elements")

    return elements
}

# Test 1: Basic HTML structure analysis
print("\nğŸ” Test 1: Basic HTML structure analysis")
elements = fetch_and_parse("https://httpbin.org/html")

if elements != none {
    # Find all headings
    h1_elements = html.find_by_tag(elements, "h1")
    print("ğŸ“ Found " + h1_elements.size().to_string() + " H1 elements")

    if h1_elements.size() > 0 {
        first_h1 = h1_elements[0]
        h1_text = html.get_text(first_h1)
        print("  ğŸ“„ First H1 text: '" + h1_text + "'")
    }

    # Find all paragraphs
    p_elements = html.find_by_tag(elements, "p")
    print("ğŸ“ Found " + p_elements.size().to_string() + " paragraph elements")

    # Find all links
    links = html.extract_links(elements)
    print("ğŸ”— Found " + links.size().to_string() + " links")

    for link in links {
        url_data = link["url"]
        text_data = link["text"]
        print("  ğŸ”— Link: '" + text_data.value + "' -> " + url_data.value)
    }
}

# Test 2: Data extraction from a structured page
print("\nğŸ“Š Test 2: Data extraction from structured content")

# Create a custom HTML page for demonstration
demo_html = "<html><head><title>Demo Page</title></head><body><div class=\"header\"><h1>Product Catalog</h1></div><div class=\"products\"><div class=\"product\" data-id=\"1\"><h3>Laptop</h3><span class=\"price\">$999.99</span><p class=\"description\">High-performance laptop</p></div><div class=\"product\" data-id=\"2\"><h3>Mouse</h3><span class=\"price\">$29.99</span><p class=\"description\">Wireless optical mouse</p></div><div class=\"product\" data-id=\"3\"><h3>Keyboard</h3><span class=\"price\">$79.99</span><p class=\"description\">Mechanical gaming keyboard</p></div></div></body></html>"

print("ğŸ”„ Parsing demo HTML content...")
demo_elements = html.parse(demo_html)

# Extract product information
products = html.find_by_class(demo_elements, "product")
print("ğŸ›’ Found " + products.size().to_string() + " products")

for product in products {
    # Get product ID
    product_id = html.get_attribute(product, "data-id")

    # Get product name
    name_elements = html.find_by_tag([product], "h3")
    name = ""
    if name_elements.size() > 0 {
        name = html.get_text(name_elements[0])
    }

    # Get price
    price_elements = html.find_by_class([product], "price")
    price = ""
    if price_elements.size() > 0 {
        price = html.get_text(price_elements[0])
    }

    # Get description
    desc_elements = html.find_by_class([product], "description")
    description = ""
    if desc_elements.size() > 0 {
        description = html.get_text(desc_elements[0])
    }

    print("  ğŸ“¦ Product " + product_id + ":")
    print("    ğŸ“› Name: " + name)
    print("    ğŸ’° Price: " + price)
    print("    ğŸ“ Description: " + description)
}

# Test 3: Text content extraction and cleaning
print("\nğŸ§¹ Test 3: Text content extraction and cleaning")

# Find elements containing specific text
price_mentions = html.find_containing_text(demo_elements, "$")
print("ğŸ’° Found " + price_mentions.size().to_string() + " elements mentioning prices")

laptop_mentions = html.find_containing_text(demo_elements, "Laptop")
print("ğŸ’» Found " + laptop_mentions.size().to_string() + " elements mentioning 'Laptop'")

# Clean text extraction
if laptop_mentions.size() > 0 {
    laptop_element = laptop_mentions[0]
    element_info = html.get_element_info(laptop_element)

    raw_text = element_info["raw_text"]
    clean_text = element_info["text"]

    print("ğŸ“ Raw text: '" + raw_text + "'")
    print("ğŸ§¹ Clean text: '" + clean_text + "'")
}

# Test 4: Form analysis
print("\nğŸ“‹ Test 4: Form analysis (using httpbin form page)")
form_elements = fetch_and_parse("https://httpbin.org/forms/post")

if form_elements != none {
    # Find all forms
    forms = html.find_by_tag(form_elements, "form")
    print("ğŸ“ Found " + forms.size().to_string() + " forms")

    # Find all input fields
    inputs = html.find_by_tag(form_elements, "input")
    print("ğŸ“¥ Found " + inputs.size().to_string() + " input fields")

    for input in inputs {
        input_type = html.get_attribute(input, "type")
        input_name = html.get_attribute(input, "name")
        print("  ğŸ“¥ Input: " + input_name + " (type: " + input_type + ")")
    }

    # Find submit buttons
    buttons = html.find_by_tag(form_elements, "button")
    selects = html.find_by_tag(form_elements, "select")
    print("ğŸ”˜ Found " + buttons.size().to_string() + " buttons")
    print("ğŸ“‹ Found " + selects.size().to_string() + " select elements")
}

# Test 5: Meta data extraction
print("\nğŸ“‹ Test 5: Meta data extraction")
if form_elements != none {
    # Extract page title
    titles = html.find_by_tag(form_elements, "title")
    if titles.size() > 0 {
        title = html.get_text(titles[0])
        print("ğŸ“„ Page title: '" + title + "'")
    }

    # Extract meta tags
    meta_tags = html.find_by_tag(form_elements, "meta")
    print("ğŸ·ï¸ Found " + meta_tags.size().to_string() + " meta tags")

    for meta in meta_tags {
        name = html.get_attribute(meta, "name")
        content = html.get_attribute(meta, "content")
        if name != "" and content != "" {
            print("  ğŸ·ï¸ Meta " + name + ": " + content)
        }
    }
}

# Test 6: Error handling in web scraping
print("\nâš ï¸ Test 6: Error handling in web scraping")

# Try to scrape a non-existent page
error_elements = fetch_and_parse("https://httpbin.org/status/404")

if error_elements == none {
    print("âœ… Error handling works correctly - no elements returned for 404")
}

# Try to parse malformed HTML
print("ğŸ”„ Testing malformed HTML parsing...")
malformed_html = "<html><head><title>Test</head><body><p>Unclosed paragraph<div>Nested incorrectly</p></div></body>"
malformed_elements = html.parse(malformed_html)
print("ğŸ“Š Parsed malformed HTML: " + malformed_elements.size().to_string() + " elements (parser recovered)")

print("\nâœ¨ Web scraping demo completed!")
print("ğŸ† Successfully demonstrated:")
print("  âœ“ HTTP requests with appropriate headers for web scraping")
print("  âœ“ HTML parsing and element extraction")
print("  âœ“ Data extraction from structured content")
print("  âœ“ Link and image extraction")
print("  âœ“ Form analysis and input field detection")
print("  âœ“ Meta data extraction (titles, meta tags)")
print("  âœ“ Text cleaning and content extraction")
print("  âœ“ Error handling for failed requests and malformed HTML")
print("\nğŸš€ Glang is ready for production web scraping applications!")